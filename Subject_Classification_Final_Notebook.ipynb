{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anustupdas/document-sucject-classification/blob/main/Subject_Classification_Final_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Notebook for Training Document Subject Classifier with Custom Bert Model.\n",
        "\n",
        "**In this notebook you can:**\n",
        "\n",
        "\n",
        "1.   Access the Data preparation, Training, Evaluation and Inferencing code.\n",
        "2.   Prepare Custom Dataset for training. [For now you can only prepare dataset for the given 56 classes present in the config file.]\n",
        "3. Training a Bert-based classifier or Finetune a existing document subject classifier for now.\n",
        "4. Run evaluation on any trainined document classifier model.\n",
        "5. Call Inference of on any pre trained  document classifier model for any given text.\n",
        "\n"
      ],
      "metadata": {
        "id": "7tG3vdM2394w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get the code base**\n",
        "\n",
        "The code base can be found\n",
        "\n",
        "\n",
        "```\n",
        "# https://github.com/anustupdas/document-sucject-classification.git\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3-N6JI7-VQJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anustupdas/document-sucject-classification.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niOqZj3e-KMi",
        "outputId": "ad367e9f-333d-4782-bd18-cf2297667ed5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'document-sucject-classification'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 88 (delta 43), reused 47 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (88/88), 826.45 KiB | 933.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy7IrLonLply",
        "outputId": "6db4a991-8e2d-4282-b1e5-da237d4ad39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Requirmened dependencies"
      ],
      "metadata": {
        "id": "v07XH1MuVcdz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5AkIO52tN4EK",
        "outputId": "bd660c2f-9fcc-4235-ad4b-7e9cec0f036f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: certifi==2023.5.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 1)) (2023.5.7)\n",
            "Collecting charset-normalizer==3.1.0 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 2))\n",
            "  Downloading charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click==8.1.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 3)) (8.1.3)\n",
            "Collecting cmake==3.26.4 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 4))\n",
            "  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy==1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: cycler==0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: filelock==3.12.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 7)) (3.12.2)\n",
            "Requirement already satisfied: fonttools==4.40.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 8)) (4.40.0)\n",
            "Requirement already satisfied: fsspec==2023.6.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 9)) (2023.6.0)\n",
            "Collecting huggingface-hub==0.15.1 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 10))\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna==3.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 11)) (3.4)\n",
            "Requirement already satisfied: Jinja2==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 12)) (3.1.2)\n",
            "Requirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 13)) (1.2.0)\n",
            "Requirement already satisfied: kiwisolver==1.4.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 14)) (1.4.4)\n",
            "Requirement already satisfied: lit==16.0.6 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 15)) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe==2.1.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 16)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib==3.7.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 17)) (3.7.1)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 18)) (1.3.0)\n",
            "Requirement already satisfied: networkx==3.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 19)) (3.1)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 20)) (3.8.1)\n",
            "Collecting numpy==1.25.0 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 21))\n",
            "  Downloading numpy-1.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 22))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 23))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 24))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 25))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 26))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 27))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 28))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 29))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 30))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 31))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 32))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging==23.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 33)) (23.1)\n",
            "Collecting pandas==2.0.2 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 34))\n",
            "  Downloading pandas-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathlib2==2.3.7.post1 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 35))\n",
            "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting Pillow==9.5.0 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 36))\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==4.23.3 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 37))\n",
            "  Downloading protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing==3.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 38)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 39)) (2.8.2)\n",
            "Collecting pytz==2023.3 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 40))\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 41)) (6.0)\n",
            "Collecting regex==2023.6.3 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 42))\n",
            "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests==2.31.0 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 43))\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors==0.3.1 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 44))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 45)) (1.2.2)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 46)) (1.10.1)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 47)) (1.16.0)\n",
            "Collecting sympy==1.12 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 48))\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-logger==0.1.0 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 49))\n",
            "  Downloading tensorboard_logger-0.1.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 50)) (3.1.0)\n",
            "Collecting tokenizers==0.13.3 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 51))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 52)) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 53)) (4.65.0)\n",
            "Collecting transformers==4.30.2 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 54))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 55)) (2.0.0)\n",
            "Requirement already satisfied: typing_extensions==4.6.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 56)) (4.6.3)\n",
            "Collecting tzdata==2023.3 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 57))\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3==2.0.3 (from -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 58))\n",
            "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 22)) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r /content/document-sucject-classification/document-subject-classification-training/requirements.txt (line 22)) (0.40.0)\n",
            "Installing collected packages: tokenizers, safetensors, pytz, cmake, urllib3, tzdata, sympy, regex, protobuf, Pillow, pathlib2, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, charset-normalizer, requests, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, tensorboard-logger, huggingface-hub, transformers\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2022.7.1\n",
            "    Uninstalling pytz-2022.7.1:\n",
            "      Successfully uninstalled pytz-2022.7.1\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.25.2\n",
            "    Uninstalling cmake-3.25.2:\n",
            "      Successfully uninstalled cmake-3.25.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.16\n",
            "    Uninstalling urllib3-1.26.16:\n",
            "      Successfully uninstalled urllib3-1.26.16\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.11.1\n",
            "    Uninstalling sympy-1.11.1:\n",
            "      Successfully uninstalled sympy-1.11.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.10.31\n",
            "    Uninstalling regex-2022.10.31:\n",
            "      Successfully uninstalled regex-2022.10.31\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.12\n",
            "    Uninstalling charset-normalizer-2.0.12:\n",
            "      Successfully uninstalled charset-normalizer-2.0.12\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.0.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.5.0 charset-normalizer-3.1.0 cmake-3.26.4 huggingface-hub-0.15.1 numpy-1.25.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-2.0.2 pathlib2-2.3.7.post1 protobuf-4.23.3 pytz-2023.3 regex-2023.6.3 requests-2.31.0 safetensors-0.3.1 sympy-1.12 tensorboard-logger-0.1.0 tokenizers-0.13.3 transformers-4.30.2 tzdata-2023.3 urllib3-2.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r /content/document-sucject-classification/document-subject-classification-training/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "86o_lFWyN_Q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5418b9f-28b3-492d-c21f-ed11a7aec3dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/document-sucject-classification/document-subject-classification-training/document_subject_classification_training\n"
          ]
        }
      ],
      "source": [
        "cd /content/document-sucject-classification/document-subject-classification-training/document_subject_classification_training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "**A prepared dataset is already available in the location**\n",
        "\n",
        "\n",
        "tr_data_path = `/content/drive/MyDrive/medior-data-scientist-case-study/training_data_all_clean_chucked`\n",
        "which is the output of `prepare_data.py` script.\n",
        "\n",
        "*One can directly feed the datalocation for training by running*\n",
        "\n",
        "\n",
        "\n",
        "`!python update_config.py --data_path tr_data_path`\n"
      ],
      "metadata": {
        "id": "EYasI7v7Xg7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative Create dataset from text files\n",
        "\n",
        "One can either create the training dataset by running the below to create training testing dataframe from raw test files.\n",
        "\n",
        "\n",
        "```\n",
        "# prepare_data.py\n",
        "```\n",
        "This code reads the raw text files from the --data_path and does the following task:\n",
        "\n",
        "\n",
        "1.   Cleans the text\n",
        "2.   Breaks a text-file into multiple sub_text_file if the number of word count is higher than threshold. default token count now is 512.\n",
        "3.  Divides the data into training validation and testing data\n",
        "By default we split the data in 70, 18, 12 [train, val, test]. One can change the split percentage by passing a parameter         `--train_split .8`\n",
        "4. creates respective dataframes [.csv] files for training testing and validation in the mention --output_data_path\n"
      ],
      "metadata": {
        "id": "04IX_tquVm1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepare_data.py --data_path \"/content/drive/MyDrive/medior-data-scientist-case-study\" --output_data_path \"/content/Training_data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5RauQJ2PrBe",
        "outputId": "f66cea92-bfb4-4adf-a474-c9ac6a4c87a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "                  Category  ...  Test\n",
            "0                Economics  ...    11\n",
            "1               Philosophy  ...     9\n",
            "2              Criminology  ...     8\n",
            "3                  Nursing  ...    11\n",
            "4        Civil Engineering  ...     7\n",
            "5                  Spanish  ...     4\n",
            "6   Mechanical Engineering  ...     7\n",
            "7               Management  ...    10\n",
            "8             Food Science  ...     8\n",
            "9             Architecture  ...    10\n",
            "10       Religious Studies  ...    10\n",
            "11             Agriculture  ...     7\n",
            "12        Entrepreneurship  ...    10\n",
            "13                   Logic  ...     6\n",
            "14                 Physics  ...    10\n",
            "15     Educational Science  ...     3\n",
            "16                 History  ...     9\n",
            "17             Probability  ...     9\n",
            "18              Psychology  ...    11\n",
            "19         Performing Arts  ...     5\n",
            "20            Econometrics  ...    11\n",
            "21                Medicine  ...    11\n",
            "22               Chemistry  ...    10\n",
            "23                   Music  ...     5\n",
            "24       Political Science  ...     7\n",
            "25                Calculus  ...     5\n",
            "26             Visual Arts  ...     2\n",
            "27            Anthropology  ...     7\n",
            "28               Geography  ...     9\n",
            "29    Chemical Engineering  ...     5\n",
            "30                 Biology  ...    11\n",
            "31               Astronomy  ...     9\n",
            "32             Linguistics  ...     3\n",
            "33  Electrical Engineering  ...     9\n",
            "34                 Algebra  ...     6\n",
            "35                Geometry  ...     9\n",
            "36           Culinary Arts  ...     2\n",
            "37            Trigonometry  ...     4\n",
            "38  Industrial Engineering  ...     6\n",
            "39        Computer Science  ...    10\n",
            "40              Literature  ...    10\n",
            "41   Environmental Science  ...     8\n",
            "42   Public Administration  ...     7\n",
            "43              Statistics  ...     8\n",
            "44                 Finance  ...    10\n",
            "45           Earth Science  ...     7\n",
            "46   Communication Science  ...     9\n",
            "47   Aerospace Engineering  ...     6\n",
            "48               Sociology  ...    10\n",
            "49      Geological Science  ...     6\n",
            "50                     Law  ...     9\n",
            "51                 English  ...     9\n",
            "52              Accounting  ...     7\n",
            "53               Dentistry  ...     2\n",
            "54       Industrial Design  ...     2\n",
            "55                  French  ...     4\n",
            "56                   Total  ...   420\n",
            "\n",
            "[57 rows x 5 columns]\n",
            "Processing data: 100% 2518/2518 [15:39<00:00,  2.68it/s]\n",
            "Processing data: 100% 703/703 [04:21<00:00,  2.69it/s]\n",
            "Processing data: 100% 420/420 [02:38<00:00,  2.65it/s]\n",
            "3\n",
            "The new directory is created at /content/Training_data!\n",
            "The new directory is created at /content/Training_data/train!\n",
            "The new directory is created at /content/Training_data/test!\n",
            "The new directory is created at /content/Training_data/dev!\n",
            "Training data have been saved in /content/Training_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python update_config.py --data_path \"/content/drive/MyDrive/medior-data-scientist-case-study/training_data_all_clean_chucked\""
      ],
      "metadata": {
        "id": "Uzm3n4rdY-QX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXoCutPhbaw8",
        "outputId": "a9559af4-21d0-4a24-b1ee-1cba0e48d2bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"tokenizer_type\": \"bert-base-multilingual-uncased\", \"model_type\": \"bert_classifier\", \"model_name\": \"bert-base-multilingual-uncased\", \"labels_class\": {\"Accounting\": 0, \"Aerospace Engineering\": 1, \"Agriculture\": 2, \"Algebra\": 3, \"Anthropology\": 4, \"Architecture\": 5, \"Astronomy\": 6, \"Biology\": 7, \"Calculus\": 8, \"Chemical Engineering\": 9, \"Chemistry\": 10, \"Civil Engineering\": 11, \"Communication Science\": 12, \"Computer Science\": 13, \"Criminology\": 14, \"Culinary Arts\": 15, \"Dentistry\": 16, \"Earth Science\": 17, \"Econometrics\": 18, \"Economics\": 19, \"Educational Science\": 20, \"Electrical Engineering\": 21, \"English\": 22, \"Entrepreneurship\": 23, \"Environmental Science\": 24, \"Finance\": 25, \"Food Science\": 26, \"French\": 27, \"Geography\": 28, \"Geological Science\": 29, \"Geometry\": 30, \"History\": 31, \"Industrial Design\": 32, \"Industrial Engineering\": 33, \"Law\": 34, \"Linguistics\": 35, \"Literature\": 36, \"Logic\": 37, \"Management\": 38, \"Mechanical Engineering\": 39, \"Medicine\": 40, \"Music\": 41, \"Nursing\": 42, \"Performing Arts\": 43, \"Philosophy\": 44, \"Physics\": 45, \"Political Science\": 46, \"Probability\": 47, \"Psychology\": 48, \"Public Administration\": 49, \"Religious Studies\": 50, \"Sociology\": 51, \"Spanish\": 52, \"Statistics\": 53, \"Trigonometry\": 54, \"Visual Arts\": 55}, \"custom_data_path\": \"/content/drive/MyDrive/medior-data-scientist-case-study/training_data_all_clean_chucked\", \"bbcdataset\": \"/data/bbc\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Subject Classification bert multilingual model"
      ],
      "metadata": {
        "id": "mdPk45jW3rHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py --custom_data --model --train --epochs 4 --bs 10 --test_bs 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwuwQl_Inda1",
        "outputId": "8427fc47-dfaa-4b8e-84da-7aa3ce51945a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading (…)solve/main/vocab.txt: 100% 872k/872k [00:00<00:00, 2.67MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 166kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 625/625 [00:00<00:00, 3.78MB/s]\n",
            "/content/drive/MyDrive/medior-data-scientist-case-study/training_data_all_clean_chucked\n",
            "File path to read:  /content/drive/MyDrive/medior-data-scientist-case-study/training_data_all_clean_chucked/train/clean_subject_classification_Train.csv\n",
            "File path to read:  /content/drive/MyDrive/medior-data-scientist-case-study/training_data_all_clean_chucked/dev/clean_subject_classification_Val.csv\n",
            "Downloading model.safetensors: 100% 672M/672M [00:01<00:00, 402MB/s]\n",
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Here is model BertClassifier(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (linear): Linear(in_features=768, out_features=56, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            " 29% 445/1536 [01:43<04:05,  4.45it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the Model"
      ],
      "metadata": {
        "id": "KhQnSV0_3xfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python update_config.py --data_path \"/content/Training_data\""
      ],
      "metadata": {
        "id": "fgxIzKjZHt-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py --custom_data --load_from '/content/drive/MyDrive/Rcnn/best_model_14.t7'"
      ],
      "metadata": {
        "id": "7lsjYp6knqyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5009a53-d23f-49c9-c295-519e716a1d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Training_data\n",
            "File path to read:  /content/Training_data/test/token_main_data_subject_classification_summary_Test.csv\n",
            "Model_Path:  /content/drive/MyDrive/Rcnn/best_model_14.t7\n",
            "Here is model BertClassifier(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (linear): Linear(in_features=768, out_features=56, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "Evaluating\n",
            "100% 2445/2445 [00:28<00:00, 86.69it/s]\n",
            "Pred: [19 19 44 ... 27 27 27]\n",
            "Original: [19 19 19 ... 27 27 27]\n",
            "Confusion Matrix\n",
            "\n",
            "[[108   0   0 ...   0   0   0]\n",
            " [  0 133   0 ...   0   0   0]\n",
            " [  0   0  12 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ...  26   0   0]\n",
            " [  0   1   0 ...   0   1   0]\n",
            " [  0   0   0 ...   0   0  24]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "            Accounting       1.00      1.00      1.00       108\n",
            " Aerospace Engineering       0.97      0.98      0.97       136\n",
            "           Agriculture       0.71      0.30      0.42        40\n",
            "               Algebra       0.73      0.79      0.76        24\n",
            "          Anthropology       0.97      0.81      0.88        36\n",
            "          Architecture       0.61      1.00      0.76        11\n",
            "             Astronomy       1.00      0.92      0.96        12\n",
            "               Biology       0.77      0.98      0.86        87\n",
            "              Calculus       1.00      0.17      0.29        12\n",
            "  Chemical Engineering       0.92      1.00      0.96       103\n",
            "             Chemistry       0.96      0.92      0.94        52\n",
            "     Civil Engineering       0.70      0.41      0.52        34\n",
            " Communication Science       0.74      0.46      0.57        56\n",
            "      Computer Science       1.00      0.98      0.99       206\n",
            "           Criminology       1.00      1.00      1.00        33\n",
            "         Culinary Arts       0.00      0.00      0.00        12\n",
            "             Dentistry       0.00      0.00      0.00         9\n",
            "         Earth Science       0.95      0.53      0.68        34\n",
            "          Econometrics       0.76      0.78      0.77        32\n",
            "             Economics       0.92      0.85      0.88        71\n",
            "   Educational Science       0.00      0.00      0.00         8\n",
            "Electrical Engineering       0.87      0.93      0.90        73\n",
            "               English       0.42      0.77      0.55        31\n",
            "      Entrepreneurship       0.41      0.75      0.53        12\n",
            " Environmental Science       0.72      0.92      0.81        52\n",
            "               Finance       0.95      1.00      0.98        41\n",
            "          Food Science       0.48      0.86      0.62        14\n",
            "                French       0.89      0.70      0.78        23\n",
            "             Geography       0.76      1.00      0.86        32\n",
            "    Geological Science       0.89      0.73      0.80        11\n",
            "              Geometry       0.74      0.97      0.84        33\n",
            "               History       0.84      0.79      0.82        39\n",
            "     Industrial Design       0.00      0.00      0.00         7\n",
            "Industrial Engineering       0.25      0.44      0.32        16\n",
            "                   Law       0.85      0.82      0.84        34\n",
            "           Linguistics       0.00      0.00      0.00         8\n",
            "            Literature       0.73      0.96      0.83        45\n",
            "                 Logic       0.94      0.77      0.85        22\n",
            "            Management       0.42      1.00      0.59        15\n",
            "Mechanical Engineering       0.88      0.86      0.87        35\n",
            "              Medicine       0.78      0.92      0.84        50\n",
            "                 Music       1.00      0.08      0.15        36\n",
            "               Nursing       0.78      0.84      0.81        61\n",
            "       Performing Arts       0.56      0.83      0.67        12\n",
            "            Philosophy       0.89      0.89      0.89        53\n",
            "               Physics       0.85      1.00      0.92        39\n",
            "     Political Science       0.55      0.81      0.65        21\n",
            "           Probability       0.98      0.97      0.98        62\n",
            "            Psychology       0.82      1.00      0.90        27\n",
            " Public Administration       0.98      0.95      0.97       274\n",
            "     Religious Studies       0.94      0.98      0.96        48\n",
            "             Sociology       0.57      0.57      0.57        23\n",
            "               Spanish       0.00      0.00      0.00        12\n",
            "            Statistics       0.79      0.96      0.87        27\n",
            "          Trigonometry       1.00      0.07      0.12        15\n",
            "           Visual Arts       1.00      0.92      0.96        26\n",
            "\n",
            "              accuracy                           0.85      2445\n",
            "             macro avg       0.72      0.71      0.68      2445\n",
            "          weighted avg       0.85      0.85      0.83      2445\n",
            "\n",
            "Test Accuracy:  0.847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencing the Model"
      ],
      "metadata": {
        "id": "-tMcAnPA32IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_inference.py --load_from \"/content/drive/MyDrive/Rcnn/best_model_14.t7\" --text \"mon misconception: if critical criminologists state that crime real then they focus on harm (crime real but sufferings are) critical criminology: whole goal is to alleviate the we need to prevent harm, not proper subject is social harm crime catch the harm that happens to groups and we are not only individualizing on criminals but our legal framework also focuses on individual victims, ignoring group and societal crimes physical (example: illness, death from medical system not a crime but causes harm) financial (if we would criminalize being billionaire, criminologists would put all their forces to study but we glorify that emotional (tougher to see and respond pandemic restrictions,..) cultural (access to informational, intellectual, cultural resources (like access to they are par\""
      ],
      "metadata": {
        "id": "zDBMdZefpEtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1956466-5ea5-4df0-fe83-ae75f85df4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_Path:  /content/drive/MyDrive/Rcnn/best_model_14.t7\n",
            "Criminology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y-90BNJW38dV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WBv0Wo1b3_S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RryCDcsP3_Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ctEMSBFdDih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohSGOsznewND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYMcSeDqe01Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "11NaB4eRx17XzFhXe5BffdXwyaqvARZe-",
      "authorship_tag": "ABX9TyOYY+HRxdTWe77sfXkwEy+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}